{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93370a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Genomic Benchmark datasets:\n",
      "1. human_enhancers_ensembl\n",
      "2. demo_coding_vs_intergenomic_seqs\n",
      "3. human_enhancers_cohn\n",
      "4. human_ocr_ensembl\n",
      "5. dummy_mouse_enhancers_ensembl\n",
      "6. human_ensembl_regulatory\n",
      "7. demo_human_or_worm\n",
      "8. human_nontata_promoters\n",
      "9. drosophila_enhancers_stark\n",
      "\n",
      "==================================================\n",
      "Starting Omni-DNA Training Pipeline\n",
      "==================================================\n",
      "\n",
      "Training Omni-DNA on human_nontata_promoters\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\BioReason\\.venv\\Lib\\site-packages\\genomic_benchmarks\\utils\\datasets.py:50: UserWarning: No version specified. Using version 0.\n",
      "  warnings.warn(f\"No version specified. Using version {metadata['version']}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "Training samples: 27097\n",
      "Test samples: 9034\n",
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OLMoForSequenceCLS were not initialized from the model checkpoint at zehui127/Omni-DNA-116M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels: 2\n",
      "Preparing datasets...\n",
      "Starting training...\n",
      "NOTE: Warning about uninitialized weights is expected - they will be trained!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13553' max='13552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13552/13552 31:33, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.245391</td>\n",
       "      <td>0.936794</td>\n",
       "      <td>0.936185</td>\n",
       "      <td>0.872546</td>\n",
       "      <td>0.937103</td>\n",
       "      <td>0.935445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.217088</td>\n",
       "      <td>0.963803</td>\n",
       "      <td>0.963616</td>\n",
       "      <td>0.927747</td>\n",
       "      <td>0.962784</td>\n",
       "      <td>0.964965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.397997</td>\n",
       "      <td>0.953730</td>\n",
       "      <td>0.953616</td>\n",
       "      <td>0.910081</td>\n",
       "      <td>0.953298</td>\n",
       "      <td>0.956789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331876</td>\n",
       "      <td>0.963914</td>\n",
       "      <td>0.963747</td>\n",
       "      <td>0.928283</td>\n",
       "      <td>0.962866</td>\n",
       "      <td>0.965421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 170\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Omni-DNA Training Pipeline\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    168\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m trainer, model, metrics = \u001b[43mtrain_omni_dna_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman_nontata_promoters\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzehui127/Omni-DNA-116M\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./omni_dna_promoter_classifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\n\u001b[32m    179\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining complete! Model saved to ./omni_dna_promoter_classifier\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 146\u001b[39m, in \u001b[36mtrain_omni_dna_classifier\u001b[39m\u001b[34m(dataset_name, model_name, output_dir, seed, learning_rate, batch_size, num_epochs, max_length)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNOTE: Warning about uninitialized weights is expected - they will be trained!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating on test set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    149\u001b[39m test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\BioReason\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\BioReason\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2812\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2810\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2811\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.load_best_model_at_end \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_model_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2812\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2814\u001b[39m \u001b[38;5;66;03m# add remaining tr_loss\u001b[39;00m\n\u001b[32m   2815\u001b[39m \u001b[38;5;28mself\u001b[39m._total_loss_scalar += tr_loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\BioReason\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3133\u001b[39m, in \u001b[36mTrainer._load_best_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3131\u001b[39m     state_dict = safetensors.torch.load_file(best_safe_model_path, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3132\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3133\u001b[39m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3134\u001b[39m     state_dict = torch.load(best_model_path, map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3136\u001b[39m \u001b[38;5;66;03m# If the model is on the GPU, it still works!\u001b[39;00m\n\u001b[32m   3137\u001b[39m \u001b[38;5;66;03m# workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963\u001b[39;00m\n\u001b[32m   3138\u001b[39m \u001b[38;5;66;03m# which takes *args instead of **kwargs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\BioReason\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1647\u001b[39m, in \u001b[36mcheck_torch_load_is_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1645\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_torch_load_is_safe\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1646\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[33m\"\u001b[39m\u001b[33m2.6\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1648\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1649\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1650\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen loading files with safetensors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1651\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1652\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, set_seed\n",
    "from torch.utils.data import Dataset\n",
    "from genomic_benchmarks.data_check import list_datasets, info\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "class GenomicDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, max_length=512):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def load_genomic_benchmark_data(dataset_name, split='train'):\n",
    "    dataset_path = download_dataset(dataset_name)\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    split_path = Path(dataset_path) / split\n",
    "    class_dirs = sorted([d for d in split_path.iterdir() if d.is_dir()])\n",
    "    \n",
    "    for label_idx, class_dir in enumerate(class_dirs):\n",
    "        for seq_file in class_dir.glob('*.txt'):\n",
    "            with open(seq_file, 'r') as f:\n",
    "                sequence = f.read().strip()\n",
    "                sequences.append(sequence)\n",
    "                labels.append(label_idx)\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "def calculate_metrics(predictions, labels):\n",
    "    valid_mask = labels != -100\n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_labels = labels[valid_mask]\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(valid_labels, valid_predictions),\n",
    "        \"f1\": sklearn.metrics.f1_score(valid_labels, valid_predictions, average=\"macro\", zero_division=0),\n",
    "        \"matthews_correlation\": sklearn.metrics.matthews_corrcoef(valid_labels, valid_predictions),\n",
    "        \"precision\": sklearn.metrics.precision_score(valid_labels, valid_predictions, average=\"macro\", zero_division=0),\n",
    "        \"recall\": sklearn.metrics.recall_score(valid_labels, valid_predictions, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "def preprocess_logits(logits, _):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    if logits.ndim == 3:\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "    return torch.argmax(logits, dim=-1)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return calculate_metrics(predictions, labels)\n",
    "\n",
    "def train_omni_dna_classifier(\n",
    "    dataset_name=\"human_nontata_promoters\",\n",
    "    model_name=\"zehui127/Omni-DNA-116M\",\n",
    "    output_dir=\"./omni_dna_output\",\n",
    "    seed=42,\n",
    "    learning_rate=5e-6,\n",
    "    batch_size=8,\n",
    "    num_epochs=4,\n",
    "    max_length=512\n",
    "):\n",
    "    print(f\"Training Omni-DNA on {dataset_name}\")\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    train_sequences, train_labels = load_genomic_benchmark_data(dataset_name, split='train')\n",
    "    test_sequences, test_labels = load_genomic_benchmark_data(dataset_name, split='test')\n",
    "    \n",
    "    num_classes = len(set(train_labels))\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Training samples: {len(train_sequences)}\")\n",
    "    print(f\"Test samples: {len(test_sequences)}\")\n",
    "    \n",
    "    print(\"Loading tokenizer and model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.model_max_length = max_length\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"Preparing datasets...\")\n",
    "    train_dataset = GenomicDataset(train_sequences, train_labels, tokenizer, max_length)\n",
    "    test_dataset = GenomicDataset(test_sequences, test_labels, tokenizer, max_length)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size * 2,\n",
    "        num_train_epochs=num_epochs,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        max_grad_norm=1.0,\n",
    "        metric_for_best_model=\"matthews_correlation\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        save_safetensors=False,\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        preprocess_logits_for_metrics=preprocess_logits,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(\"NOTE: Warning about uninitialized weights is expected - they will be trained!\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL TEST RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return trainer, model, test_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Available Genomic Benchmark datasets:\")\n",
    "    datasets = list_datasets()\n",
    "    for i, ds in enumerate(datasets):\n",
    "        print(f\"{i+1}. {ds}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting Omni-DNA Training Pipeline\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    trainer, model, metrics = train_omni_dna_classifier(\n",
    "        dataset_name=\"human_nontata_promoters\",\n",
    "        model_name=\"zehui127/Omni-DNA-116M\",\n",
    "        output_dir=\"./omni_dna_promoter_classifier\",\n",
    "        seed=42,\n",
    "        learning_rate=5e-6,\n",
    "        batch_size=8,\n",
    "        num_epochs=4,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete! Model saved to ./omni_dna_promoter_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac65d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a65b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa4999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206537b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa62f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5513cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
