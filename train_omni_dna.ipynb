{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93370a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CUDA/GPU CONFIGURATION\n",
      "======================================================================\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "\n",
      "✅ GPU ENABLED!\n",
      "GPU Name: NVIDIA GeForce RTX 4090\n",
      "GPU Count: 1\n",
      "Device: cuda\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CUDA and GPU Availability Check\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CUDA/GPU CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n✅ GPU ENABLED!\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Device: {device}\")\n",
    "else:\n",
    "    print(\"\\n❌ CUDA not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "        \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3a65b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Genomic Benchmark datasets:\n",
      "1. human_enhancers_ensembl\n",
      "2. dummy_mouse_enhancers_ensembl\n",
      "3. human_ensembl_regulatory\n",
      "4. demo_coding_vs_intergenomic_seqs\n",
      "5. drosophila_enhancers_stark\n",
      "6. human_nontata_promoters\n",
      "7. human_enhancers_cohn\n",
      "8. demo_human_or_worm\n",
      "9. human_ocr_ensembl\n",
      "\n",
      "==================================================\n",
      "Starting Omni-DNA Training Pipeline\n",
      "==================================================\n",
      "\n",
      "Training Omni-DNA on human_nontata_promoters\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\T2430392\\Miniconda3\\envs\\omni_dna_fixed\\lib\\site-packages\\genomic_benchmarks\\utils\\datasets.py:50: UserWarning: No version specified. Using version 0.\n",
      "  warnings.warn(f\"No version specified. Using version {metadata['version']}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "Training samples: 27097\n",
      "Test samples: 9034\n",
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OLMoForSequenceCLS were not initialized from the model checkpoint at zehui127/Omni-DNA-116M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels: 2\n",
      "Preparing datasets...\n",
      "Starting training...\n",
      "NOTE: Warning about uninitialized weights is expected - they will be trained!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16261' max='16260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16260/16260 25:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.291423</td>\n",
       "      <td>0.940115</td>\n",
       "      <td>0.939489</td>\n",
       "      <td>0.879333</td>\n",
       "      <td>0.940899</td>\n",
       "      <td>0.938438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.259147</td>\n",
       "      <td>0.963693</td>\n",
       "      <td>0.963512</td>\n",
       "      <td>0.927634</td>\n",
       "      <td>0.962656</td>\n",
       "      <td>0.964981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.294070</td>\n",
       "      <td>0.967014</td>\n",
       "      <td>0.966834</td>\n",
       "      <td>0.934086</td>\n",
       "      <td>0.966035</td>\n",
       "      <td>0.968053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 173\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Omni-DNA Training Pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m trainer, model, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_omni_dna_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman_nontata_promoters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzehui127/Omni-DNA-116M\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./omni_dna_promoter_classifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\n\u001b[0;32m    182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete! Model saved to ./omni_dna_promoter_classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 149\u001b[0m, in \u001b[0;36mtrain_omni_dna_classifier\u001b[1;34m(dataset_name, model_name, output_dir, seed, learning_rate, batch_size, num_epochs, max_length)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOTE: Warning about uninitialized weights is expected - they will be trained!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 149\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset)\n",
      "File \u001b[1;32mc:\\Users\\T2430392\\Miniconda3\\envs\\omni_dna_fixed\\lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\T2430392\\Miniconda3\\envs\\omni_dna_fixed\\lib\\site-packages\\transformers\\trainer.py:2812\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2810\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mload_best_model_at_end \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2812\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[38;5;66;03m# add remaining tr_loss\u001b[39;00m\n\u001b[0;32m   2815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_loss_scalar \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\T2430392\\Miniconda3\\envs\\omni_dna_fixed\\lib\\site-packages\\transformers\\trainer.py:3133\u001b[0m, in \u001b[0;36mTrainer._load_best_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3131\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m safetensors\u001b[38;5;241m.\u001b[39mtorch\u001b[38;5;241m.\u001b[39mload_file(best_safe_model_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3133\u001b[0m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3134\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(best_model_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3136\u001b[0m \u001b[38;5;66;03m# If the model is on the GPU, it still works!\u001b[39;00m\n\u001b[0;32m   3137\u001b[0m \u001b[38;5;66;03m# workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963\u001b[39;00m\n\u001b[0;32m   3138\u001b[0m \u001b[38;5;66;03m# which takes *args instead of **kwargs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\T2430392\\Miniconda3\\envs\\omni_dna_fixed\\lib\\site-packages\\transformers\\utils\\import_utils.py:1647\u001b[0m, in \u001b[0;36mcheck_torch_load_is_safe\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_torch_load_is_safe\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1647\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1648\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1649\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1650\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading files with safetensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1652\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, set_seed\n",
    "from torch.utils.data import Dataset\n",
    "from genomic_benchmarks.data_check import list_datasets, info\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "class GenomicDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, max_length=512):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def load_genomic_benchmark_data(dataset_name, split='train'):\n",
    "    dataset_path = download_dataset(dataset_name)\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    split_path = Path(dataset_path) / split\n",
    "    class_dirs = sorted([d for d in split_path.iterdir() if d.is_dir()])\n",
    "    \n",
    "    for label_idx, class_dir in enumerate(class_dirs):\n",
    "        for seq_file in class_dir.glob('*.txt'):\n",
    "            with open(seq_file, 'r') as f:\n",
    "                sequence = f.read().strip()\n",
    "                sequences.append(sequence)\n",
    "                labels.append(label_idx)\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "def calculate_metrics(predictions, labels):\n",
    "    valid_mask = labels != -100\n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_labels = labels[valid_mask]\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(valid_labels, valid_predictions),\n",
    "        \"f1\": sklearn.metrics.f1_score(valid_labels, valid_predictions, average=\"macro\", zero_division=0),\n",
    "        \"matthews_correlation\": sklearn.metrics.matthews_corrcoef(valid_labels, valid_predictions),\n",
    "        \"precision\": sklearn.metrics.precision_score(valid_labels, valid_predictions, average=\"macro\", zero_division=0),\n",
    "        \"recall\": sklearn.metrics.recall_score(valid_labels, valid_predictions, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "def preprocess_logits(logits, _):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    if logits.ndim == 3:\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "    return torch.argmax(logits, dim=-1)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return calculate_metrics(predictions, labels)\n",
    "\n",
    "def train_omni_dna_classifier(\n",
    "    dataset_name=\"human_nontata_promoters\",\n",
    "    model_name=\"zehui127/Omni-DNA-116M\",\n",
    "    output_dir=\"./omni_dna_output\",\n",
    "    seed=42,\n",
    "    learning_rate=5e-6,\n",
    "    batch_size=8,\n",
    "    num_epochs=3,\n",
    "    max_length=512\n",
    "):\n",
    "    print(f\"Training Omni-DNA on {dataset_name}\")\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    train_sequences, train_labels = load_genomic_benchmark_data(dataset_name, split='train')\n",
    "    test_sequences, test_labels = load_genomic_benchmark_data(dataset_name, split='test')\n",
    "    \n",
    "    num_classes = len(set(train_labels))\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Training samples: {len(train_sequences)}\")\n",
    "    print(f\"Test samples: {len(test_sequences)}\")\n",
    "    \n",
    "    print(\"Loading tokenizer and model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.model_max_length = max_length\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "        trust_remote_code=True\n",
    "    ).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    \n",
    "    print(\"Preparing datasets...\")\n",
    "    train_dataset = GenomicDataset(train_sequences, train_labels, tokenizer, max_length)\n",
    "    test_dataset = GenomicDataset(test_sequences, test_labels, tokenizer, max_length)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size * 2,\n",
    "        num_train_epochs=num_epochs,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        max_grad_norm=1.0,\n",
    "        metric_for_best_model=\"matthews_correlation\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        save_safetensors=False,\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        preprocess_logits_for_metrics=preprocess_logits,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(\"NOTE: Warning about uninitialized weights is expected - they will be trained!\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL TEST RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return trainer, model, test_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Available Genomic Benchmark datasets:\")\n",
    "    datasets = list_datasets()\n",
    "    for i, ds in enumerate(datasets):\n",
    "        print(f\"{i+1}. {ds}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting Omni-DNA Training Pipeline\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    trainer, model, metrics = train_omni_dna_classifier(\n",
    "        dataset_name=\"human_nontata_promoters\",\n",
    "        model_name=\"zehui127/Omni-DNA-116M\",\n",
    "        output_dir=\"./omni_dna_promoter_classifier\",\n",
    "        seed=42,\n",
    "        learning_rate=5e-6,\n",
    "        batch_size=5,\n",
    "        num_epochs=3,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete! Model saved to ./omni_dna_promoter_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa4999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206537b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa62f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5513cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omni_dna_fixed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
